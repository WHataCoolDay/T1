{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6.3",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPrPKRgYHyHb9sD3ZOFYB5s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WHataCoolDay/T1/blob/main/6_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OoMFDmb1wUVr",
        "outputId": "b1ad68ec-9d5f-4bb9-e77c-b86a4d66c4e1"
      },
      "source": [
        "cd C:\\Users\\lwj98\\Python"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'C:Userslwj98Python'\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "kYo3tezywd20",
        "outputId": "f80f0031-e8b5-40ca-b513-13459456f359"
      },
      "source": [
        "wget https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-51c815cf4a05>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    wget https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWi-mA0QwlU1"
      },
      "source": [
        "import os\n",
        "\n",
        "data_dir = './datasets/jena_climate'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aWzY0kxx632"
      },
      "source": [
        "fname = os.path.join(data_dir, 'jena_climate_2009_2016.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        },
        "id": "bAzurALuyDT-",
        "outputId": "2a1cb35f-851c-4af6-e6a9-7f9a4e9c2860"
      },
      "source": [
        "f = open(fname)\n",
        "data = f.read()\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-bad0c46ffde9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './datasets/jena_climate/jena_climate_2009_2016.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQEzCbt7yISU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3O9Ku0wKZgz",
        "outputId": "d7bef0c3-e188-41a8-e65f-c1983c9fd4f3"
      },
      "source": [
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "max_features = 10000\n",
        "max_len = 500\n",
        "\n",
        "print('데이터 로드...')\n",
        "(x_train, y_train),(x_test,y_test) = imdb.load_data(num_words=max_features)\n",
        "print(len(x_train), '훈련 시퀀스')\n",
        "print(len(x_test), '테스트 시퀀스')\n",
        "\n",
        "print('시퀀스 패딩 (sample x time')\n",
        "x_train = sequence.pad_sequences(x_train, maxlen = max_len)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen = max_len)\n",
        "print('x_train 크기: ', x_train.shape)\n",
        "print('x_test 크기: ', x_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "데이터 로드...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/keras/datasets/imdb.py:155: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.7/dist-packages/keras/datasets/imdb.py:156: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "25000 훈련 시퀀스\n",
            "25000 테스트 시퀀스\n",
            "시퀀스 패딩 (sample x time\n",
            "x_train 크기:  (25000, 500)\n",
            "x_test 크기:  (25000, 500)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUC3HoE2LPiB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cae19961-18b4-4290-f97e-1807d5541ff5"
      },
      "source": [
        "import os\n",
        "os.getcwd()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mb9WMdZxN2GK"
      },
      "source": [
        "data_dir = 'C:/Users/lwj98/Python/jena_climate'\n",
        "fname = os.path.join(data_dir, 'jena_climate_2009_2016.csv')\n",
        "\n",
        "f = open(fname)\n",
        "data = f.read()\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xo24EU-UN2rQ"
      },
      "source": [
        "lines = data.split('\\n')\n",
        "header = lines[0].split(',')\n",
        "lines = lines[1:]\n",
        "\n",
        "print(header)\n",
        "print(len(lines))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2oCWIO6N2tX"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "float_data = np.zeros((len(lines), len(header)-1))\n",
        "for i,line in enumerate(lines):\n",
        "    values = [float(x) for x in line.split(',')[1:]]\n",
        "    float_data[i,:] = values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JY7idVYN20z"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "temp = float_data[:,1]\n",
        "plt.plot(range(len(temp)),temp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlN3DZ2hN-5S"
      },
      "source": [
        "plt.plot(range(1440), temp[:1440])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYyAZ3n9N-7L"
      },
      "source": [
        "mean = float_data[:200000].mean(axis = 0)\n",
        "float_data -= mean\n",
        "std = float_data[:200000].std(axis = 0)\n",
        "float_data /= std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7CfJIP6N-9E"
      },
      "source": [
        "def generator(data, lookback, delay, min_index, max_index, shuffle=False, batch_size = 128, step = 6):\n",
        "    if max_index is None:\n",
        "        max_index = len(data) - delay - 1\n",
        "    i = min_index + lookback\n",
        "    while 1:\n",
        "        if shuffle:\n",
        "            rows = np.random.randint(min_index + lookback, max_index, size = batch_size)\n",
        "        else:\n",
        "            if i + batch_size >= max_index:\n",
        "                i = min_index + lookback\n",
        "            rows = np.arrange(i, min(i + batch_size, max_index))\n",
        "            i += len(rows)\n",
        "            \n",
        "        samples = np.zeros((len(rows), lookback // step, data.shape[-1]))\n",
        "        targets = np.zeros((len(rows),))\n",
        "        for j, row in enumerate(rows):\n",
        "            indices = range(rows[j] - lookback, rows[j], step)\n",
        "            samples[j] = data[indices]\n",
        "            targets[j] = data[rows[j] + delay][1]\n",
        "        yield samples, targets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0ohy5i6N--q"
      },
      "source": [
        "lookback = 1440\n",
        "step = 6\n",
        "delay = 144\n",
        "batch_size = 128\n",
        "train_gen = generator(float_data, lookback = lookback, delay = delay, min_index = 0, max_index = 200000, shuffle = True, step = step, batch_size = batch_size)\n",
        "val_gen = generator(float_data, lookback = lookback, delay = delay, min_index = 200001, max_index = 300000, shuffle = True, step = step, batch_size = batch_size)\n",
        "test_getn = generator(float_data, lookback = lookback, delay = delay, min_index = 300001, max_index = None, shuffle = True, step = step, batch_size = batch_size)\n",
        "\n",
        "val_steps = (300000 - 200001 - lookback) // batch_size\n",
        "test_steps = (len(float_data) - 300001 - lookback) // batch_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rW9LonEZN_Am"
      },
      "source": [
        "# 6-35 상식적인 기준 모델의 MAE 계산하기\n",
        "def evaluate_naive_method():\n",
        "    batch_maes = []\n",
        "    for step in range(val_steps):\n",
        "        samples, targets = next(val_gen)\n",
        "        preds = samples[:, -1, 1]\n",
        "        mae = np.mean(np.abs(preds - targets))\n",
        "        batch_maes.append(mae)\n",
        "    print(np.mean(batch_maes))\n",
        "    \n",
        "evaluate_naive_method()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPKPr1eXODT2"
      },
      "source": [
        "celsius_mae = 0.29 * std[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVH4ICESODVu"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Flatten(input_shape=(lookback // step, float_data.shape[-1])))\n",
        "model.add(layers.Dense(32, activation = 'relu'))\n",
        "model.add(layers.Dense(1))\n",
        "model.compile(optimizer = RMSprop(), loss = 'mae')\n",
        "history = model.fit_generator(train_gen, steps_per_epoch = 500, epochs = 20, validation_data = val_gen, validation_steps = val_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiMcIHtiODXt"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(loss) + 1)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label = 'Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label = 'Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uH-jtr47OGjb"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.GRU(32, input_shape = (None, float_data.shape[-1])))\n",
        "model.add(layers.Dense(1))\n",
        "\n",
        "model.compile(optimizer = RMSprop(), loss='mae')\n",
        "history = model.fit_generator(train_gen, steps_per_epoch = 500, epochs = 20, validation_data = val_gen, validation_steps = val_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "810wecgUOGlV"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layer.GRU(32, dropout = 0.1, recurrent_dropout = 0.5, return_sequences = True, input_shape=(None, float_data.shape[-1])))\n",
        "model.add(layers.GRU(64, activation = 'relu', dropout=0.1, recurrent_dropout=0.5))\n",
        "model.add(Dense(1))\n",
        "\n",
        "model.compile(optimizer=RMSprop(), loss='mae')\n",
        "history = model.fit_generator(train_gen, steps_per_epoch = 500, epochs = 40, validation_data = val_gen, validation_steps = val_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_X9i-UHOGnU"
      },
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras import layers\n",
        "from keras import Input\n",
        "\n",
        "seq_model = Sequential()\n",
        "seq_model.add(layers.Dense(32,activation='relu', input_shape=(64,)))\n",
        "seq_model.add(layers.Dense(32,activation='relu'))\n",
        "seq_model.add(layers.Dense(10,activation='softmax'))\n",
        "\n",
        "input_tensor = Input(shape=(64,))\n",
        "x = layers.Dense(32, activation='relu')(input_tensor)\n",
        "x = layers.Dense(32, activation='relu')(x)\n",
        "output_tensor = layers.Dense(10, activation='softmax')(x)\n",
        "\n",
        "model = Model(input_tensor, output_tensor)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnSAHbESOGp8"
      },
      "source": [
        "from keras.models import Model\n",
        "from keras import layers\n",
        "from keras import Input\n",
        "\n",
        "text_vocabulary_size = 10000\n",
        "question_vocabulary_size = 10000\n",
        "answer_vocabulary_size = 500\n",
        "\n",
        "text_input = Input(shape=(None,), dtype = 'int32', name = 'text')\n",
        "embedded_text = layers.Embedding(text_vocabulary_size,64)(text_input)\n",
        "encoded_text = layers.LSTM(32)(embedded_text)\n",
        "question_input = Input(shape=(None,),dtype = 'int32', name='question')\n",
        "\n",
        "embedded_question = layers.Embedding(question_vocabulary_size, 32)(question_input)\n",
        "encoded_question = layers.LSTM(16)(embedded_question)\n",
        "\n",
        "concatenated = layers.concatenate([encoded_text, encoded_question], axis = -1)\n",
        "\n",
        "answer = layers.Dense(answer_vocabulary_size, activation='softmax')(concatenated)\n",
        "\n",
        "model = Model([text_input, question_input],answer)\n",
        "model.compile(optimizer = 'rmsprop', loss = 'categorial_crossentropy', metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkVOVjseOLl7"
      },
      "source": [
        "import numpy as np\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "num_samples = 1000\n",
        "max_length = 100\n",
        "\n",
        "text = np.random.randint(1,text_vocabulary_size, size = (num_samples, max_length))\n",
        "question = np.random.randint(1, question_vocabulary_size, size = (num_samples, max_length))\n",
        "answers = np.random.randint(0, answer_vocabulary_size, size = num_samples)\n",
        "\n",
        "answers = to_categorical(answers)\n",
        "model.fit([text, question], answers, epochs=10, batch_size = 128)\n",
        "model.fit({'text':text, 'question': question}, answers, epochs=10, batch_size = 128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GykFIHHOLn3"
      },
      "source": [
        "from keras import layers\n",
        "from keras import Input\n",
        "from keras.models import Model\n",
        "vocabulary_size = 50000\n",
        "num_income_groups = 10\n",
        "\n",
        "posts_input = Input(shape=(None,), dtype = 'int32', name = 'posts')\n",
        "embedded_posts = layers.Embedding(vocabulary_size, 256)(posts_input)\n",
        "x = layers.Conv1D(128,5, activation = 'relu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIXUjn5aOL7s"
      },
      "source": [
        "import keras\n",
        "\n",
        "callback_list = [\n",
        "    keras.callbacks.EarlyStopping(\n",
        "    monitor = 'val_acc'\n",
        "    patience = 1,\n",
        "    ),\n",
        "    keras.callbacks.ModelCheckPoint(\n",
        "    filepath = 'my_model.h5'\n",
        "    monitor = 'val_loss'\n",
        "    save_best_only = True,)\n",
        "]\n",
        "\n",
        "model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics = ['acc'])\n",
        "model.fit(x,y,epochs = 10, batch_size=32, callbacks= callback_list, validation_data=(x_val,y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_6q6njYOL-E"
      },
      "source": [
        "import keras\n",
        "from keras import layers\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "max_features = 2000\n",
        "max_len = 500\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words = max_features)\n",
        "x_train = sequence.pad_sequences(x_train, maxlen = max_len)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen = max_len)\n",
        "\n",
        "model=keras.models.Sequential()\n",
        "model.add(layers.Embedding(max_features, 128, input_length = max_len, name = 'embed'))\n",
        "model.add(layers.Conv1D(32, 7, activation = 'relu'))\n",
        "model.add(layers.MaxPooling1D(5))\n",
        "model.add(layers.Conv1D(32,7, activation = 'relu'))\n",
        "model.add(layers.GlobalMaxPooling1D())\n",
        "model.add(layers.Dense(1))\n",
        "model.summary()\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics = ['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPoJAGhtOQOd"
      },
      "source": [
        "mkdir my_log_dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWrpbIF4OQQY"
      },
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.TensorBoard(\n",
        "    log_dir = 'my_log_dir',\n",
        "    histogram_freq = 1,\n",
        "    embeddings_freq = 1,\n",
        "    )\n",
        "]\n",
        "history = model.fit(x_train,y_train,epochs=20,batch_size=128,validation_split=0.2,callbacks=callbacks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pf9CvicMOQSM"
      },
      "source": [
        "tensorboard --logdir = my_log_dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFiRrh9UOQWR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}